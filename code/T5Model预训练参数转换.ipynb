{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nT5Model预训练参数转换\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"\"\"\n",
    "T5Model预训练参数转换\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, mindspore\n",
    "import numpy as np\n",
    "from transformers.models.t5 import modeling_t5 as pt\n",
    "import mindnlp.models.t5 as m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"/home/daiyuxin/ljm_script/codespace/t5_ckpt/t5-small\"\n",
    "config_path = f\"{path}/t5-small_config.json\"\n",
    "pytorch_model_path = f\"{path}/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'], unexpected_keys=['decoder.block.0.layer.1.EncDecAttention.relative_attention_bias.weight'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init config\n",
    "import json\n",
    "with open(config_path, encoding='utf-8') as config:\n",
    "    config = json.load(config)\n",
    "\n",
    "pt_config = pt.T5Config(**config)\n",
    "pt_dict = torch.load(pytorch_model_path)\n",
    "pt_model = pt.T5Model(pt_config)\n",
    "pt_model.load_state_dict(pt_dict, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight\n",
      "encoder.embed_tokens.weight\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "encoder.final_layer_norm.weight\n",
      "decoder.embed_tokens.weight\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "# print pt_model parameters' name\n",
    "pt_params = pt_model.state_dict()\n",
    "for key in pt_params.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init config\n",
    "ms_config = m.T5Config(**config)\n",
    "ms_model = m.T5Model(ms_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder.embed_tokens.embedding_table\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.embedding_table\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "encoder.final_layer_norm.weight\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.embedding_table\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "# print ms_model parameters' name\n",
    "for key, param in ms_model.parameters_and_names():\n",
    "    print(param.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def torch_to_mindspore(pth_file, size:str=None):\n",
    "    try:\n",
    "        import torch\n",
    "    except:\n",
    "        raise ImportError(f\"'import torch' failed, please install torch by \"\n",
    "                          f\"`pip install torch` or instructions from 'https://pytorch.org'\")\n",
    "\n",
    "    size = \"mindspore\" if not size else size # rename ckpt\n",
    "\n",
    "    from mindspore import Tensor\n",
    "    from mindspore.train.serialization import save_checkpoint\n",
    "\n",
    "    logging.info('Starting checkpoint conversion.')\n",
    "    ms_ckpt = []\n",
    "    state_dict = torch.load(pth_file, map_location=torch.device('cpu'))\n",
    "\n",
    "    for k, v in state_dict.items():\n",
    "        if 'shared.weight' in k:\n",
    "            k = k.replace('shared.weight', 'decoder.embed_tokens.embedding_table')\n",
    "        if 'relative_attention_bias.weight' in k:\n",
    "            k = k.replace('relative_attention_bias.weight', 'relative_attention_bias.embedding_table')\n",
    "        ms_ckpt.append({'name': k, 'data': Tensor(v.numpy())})\n",
    "\n",
    "    ms_ckpt_path = pth_file.replace('.bin','.ckpt')\n",
    "    ms_ckpt_path = ms_ckpt_path.replace('pytorch',size)\n",
    "    try:\n",
    "        save_checkpoint(ms_ckpt, ms_ckpt_path)\n",
    "    except:\n",
    "        raise RuntimeError(f'Save checkpoint to {ms_ckpt_path} failed, please checkout the path.')\n",
    "\n",
    "    return ms_ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/daiyuxin/ljm_script/codespace/t5_ckpt/t5-small/t5-small_model.ckpt'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_to_mindspore(\"/home/daiyuxin/ljm_script/codespace/t5_ckpt/t5-small/pytorch_model.bin\", \"t5-small\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ljm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
